{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recsim_Tutorial_IIC3633-2_2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9PTfi4n7oS6"
      },
      "source": [
        "# Práctico Recsim\n",
        "\n",
        "Adaptado de los tutoriales disponibles en: https://github.com/google-research/recsim por Manuel Cartagena."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeT1yUJSOjDh"
      },
      "source": [
        "# Install Recsim\n",
        "!pip install --upgrade --no-cache-dir recsim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVOrpYsfi_kq"
      },
      "source": [
        "## Reinforcement Learning\n",
        "\n",
        "![RL setup](https://github.com/bamine/recsys-summer-school/raw/12e57cc4fd1cb26164d2beebf3ca29ebe2eab960/notebooks/images/rl-setup.png)\n",
        "\n",
        "\n",
        "## Tipos de interacción\n",
        "\n",
        "![texto alternativo](https://github.com/bamine/recsys-summer-school/raw/12e57cc4fd1cb26164d2beebf3ca29ebe2eab960/notebooks/images/organic-bandit.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hhwKK1fOqq1"
      },
      "source": [
        "## Importar paquetes necesarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpCXt6tkYD_w"
      },
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "from recsim import document\n",
        "from recsim import user\n",
        "from recsim.choice_model import MultinomialLogitChoiceModel\n",
        "from recsim.simulator import environment\n",
        "from recsim.simulator import recsim_gym\n",
        "from recsim.simulator import runner_lib\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94GLvAnijJIm"
      },
      "source": [
        "## Recsim\n",
        "![RecSim implementation](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/simulator_implemented.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8cuzWjdj3oz"
      },
      "source": [
        "# Resumen\n",
        "\n",
        "Un paso en la simulación de Recsim consiste en:\n",
        "\n",
        "\n",
        "1.   La Base de Datos de Documentos (items) provee un corpus de *D* documentos al recomendador.\n",
        "2.   El recomendador observa los *D* documentos (y sus features) junto a las respuestas del usuario para la última recomendación. Luego hace una selección ordenada de *k* documentos para presentárselos al usuario.\n",
        "3.   El usuario examina la lista y escoge a lo más un documento (no escoger uno es una opción). Esto genera una transición del estado del usuario. Finalmente el usuario emite una observación del documento, que en la siguiente iteración el recomendador podrá ver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT2pJkTf3Io7"
      },
      "source": [
        "# Escenario de la simulación: Videos de Memes vs Educativos\n",
        "\n",
        "Los documentos de nuestro corpus corresponderan a items (en este caso videos) que se caracterizan por su grado de educativo o de meme. Documentos \"meme\" generan alto compromiso (**engagement**), pero _hipotéticamente_ el consumo a largo plazo de estos documentos lleva a disminuir la satisfacción del usuario. Por otro lado, documentus educativos generan relativamente bajo engagement, pero su consumo conlleva a una mayor satisfacción a largo plazo. Modelaremos esta propiedad de los documentos como una feature continua que puede tomar valores entre [0,1], le llamaremos Educativeness-scale. Un documento con score 1 es totalmente educativo, mientras que un document con score 0 es totalmente meme.\n",
        "\n",
        "El estado latente del usuario consiste en una variable de dimensión 1 llamada *satisfacción*. Cada vez que consume un documento \"educativo\", esta variable tiende a incrementar, y opuestamente, un documento meme tiende a disminuir la satisfacción.\n",
        "\n",
        "Al consumir un documento, el usuario emite una medida estocástica del engagement (tiempo que ve el video) sobre el documento. Este valor es proporcional a la satisfacción del usuario e inversamente proporcional a la educatividad del documento en cuestión.\n",
        "\n",
        "Por lo tanto, el objetivo es encontrar el mix óptimo de documentos para mantener el engagement del usuario por un período largo de tiempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDdSxkJjBmN5"
      },
      "source": [
        "## Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifnex8kHBrZx"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsSuXHgNOyvl"
      },
      "source": [
        "Clase que define los documentos, LTS es una abreviación de Long Term Satisfaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0zLUsmXBcM-"
      },
      "source": [
        "class LTSDocument(document.AbstractDocument):\n",
        "  def __init__(self, doc_id, educativeness, cluster_id):\n",
        "    self.educativeness = educativeness\n",
        "    self.cluster_id = cluster_id\n",
        "    # doc_id es un ID unico para el documento\n",
        "    super(LTSDocument, self).__init__(doc_id)\n",
        "\n",
        "  NUM_CLUSTERS = 4\n",
        "\n",
        "  # Una observación son los valores públicos del documento\n",
        "  def create_observation(self):\n",
        "    return {'educativeness': np.array(self.educativeness), 'cluster_id': self.cluster_id}\n",
        "\n",
        "  # El espacio de la observación utiliza la el estándar del gym de OpenAI: https://gym.openai.com/docs/#spaces\n",
        "  @classmethod\n",
        "  def observation_space(self):\n",
        "    return spaces.Dict({\n",
        "      'educativeness': spaces.Box(shape=(1,), dtype=np.float32, low=0.0, high=1.0),\n",
        "      'cluster_id': spaces.Discrete(self.NUM_CLUSTERS)\n",
        "    })\n",
        "  \n",
        "  # Método para definir cómo se imprime un documento\n",
        "  def __str__(self):\n",
        "    return \"Document {} from cluster {} with educativeness {}.\".format(self._doc_id, self.cluster_id, self.educativeness)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7aiBraXBpH2"
      },
      "source": [
        "### Sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVFC_Z5oPfGL"
      },
      "source": [
        "Un Sampler es una clase que creará una instancia del objeto en cuestión, en este caso para los documentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHM9XL-1Bc1i"
      },
      "source": [
        "class LTSDocumentSampler(document.AbstractDocumentSampler):\n",
        "  def __init__(self, doc_ctor=LTSDocument, **kwargs):\n",
        "    super(LTSDocumentSampler, self).__init__(doc_ctor, **kwargs)\n",
        "    self._doc_count = 0\n",
        "\n",
        "  def sample_document(self):\n",
        "    doc_features = {}\n",
        "    doc_features['doc_id'] = self._doc_count\n",
        "    doc_features['educativeness'] = self._rng.random_sample()\n",
        "    doc_features['cluster_id'] = self._rng.choice(self._doc_ctor.NUM_CLUSTERS)\n",
        "    self._doc_count += 1\n",
        "    return self._doc_ctor(**doc_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S02-Non-PqXD"
      },
      "source": [
        "Ejemplo de sampleo de documentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZHSsTUPy51Y"
      },
      "source": [
        "sampler = LTSDocumentSampler()\n",
        "for i in range(5): print(sampler.sample_document())\n",
        "d = sampler.sample_document()\n",
        "print(\"Documents have observation space:\", d.observation_space(), \"\\n\"\n",
        "      \"An example realization is: \", d.create_observation())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW_jfP8-BxJJ"
      },
      "source": [
        "## User"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fU1KAmX8GeU"
      },
      "source": [
        "El modelo de usuario para este tutorial es:\n",
        "* Cada usuario tiene una feature llamada net educativeness exposure ($\\text{nee}_t$), y satisfacción ($\\text{sat}_t$). Están relacionadas mediante una función logística para reflejar que la satisfacción no puede no tener un límite.\n",
        "$$\\text{sat}_t = \\sigma(\\tau\\cdot\\text{nee}_t),$$\n",
        "donde $\\tau$ es un parámetro de sensitividad específico por usuario.\n",
        "* Dado un slate $S$, el usuario escoge un item basado en un modelo de decisión multinomial con la educativeness como feature: $p(\\text{usuario escoja }d_i \\text{ del slate }S) \\sim e^{1-\\mathrm{educativeness}(d_i)}$\n",
        "* Una vez el usuario escoge un documento, la net educativeness exposure evoluciona de la manera:\n",
        "$$\\text{nee}_{t+1} = \\beta \\cdot \\text{nee}_t + 2(k_d - 1/2) + {\\cal N}(0, \\eta),$$\n",
        "donde $\\beta$ es un factor específico por usuario que llamaremos memory discount (factor de olvido), $k_d$ es la educativeness del documento escogido y $\\eta$ es ruido proveniente de una distribución normal que llamaremos innovación (innovation).\n",
        "* Finalmente, el usuario interactúa con el contenido escogido por $s_d$ segundos, donde $s_d$ es sacado de alguna distribución\n",
        "$$s_d\\sim\\log{\\cal N}(k_d\\mu_k + (1-k_d)\\mu_c, k_d\\sigma_k + (1-k_d)\\sigma_c),$$\n",
        "por ejemplo, una distribución log-normal con interpolando linealmente entre una respuesta puramente educativa $(\\mu_k, \\sigma_k)$ y una respuesta puramente meme $(\\mu_c, \\sigma_c)$.\n",
        "\n",
        "De acuerdo a esto, el estado de un usuario está definido por la tupla $(\\text{sat}, \\tau, \\beta, \\eta, \\mu_k, \\sigma_k, \\mu_c, \\sigma_c).$ La satisfacción es la única variable dinámica del estado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAqJN4J1BzPH"
      },
      "source": [
        "### State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzd5qrcdPtgw"
      },
      "source": [
        "Esta clase maneja el estado del usuario durante una simulación, tanto las variables públicas como privadas de este durante el tiempo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPMnZJGyBe3U"
      },
      "source": [
        "class LTSUserState(user.AbstractUserState):\n",
        "  def __init__(self, memory_discount, sensitivity, innovation_stddev,\n",
        "               meme_mean, meme_stddev, educ_mean, educ_stddev,\n",
        "               net_educativeness_exposure, time_budget, observation_noise_stddev=0.1\n",
        "              ):\n",
        "    ## Transition model parameters\n",
        "    self.memory_discount = memory_discount\n",
        "    self.sensitivity = sensitivity\n",
        "    self.innovation_stddev = innovation_stddev\n",
        "\n",
        "    ## Engagement parameters\n",
        "    self.meme_mean = meme_mean\n",
        "    self.meme_stddev = meme_stddev\n",
        "    self.educ_mean = educ_mean\n",
        "    self.educ_stddev = educ_stddev\n",
        "\n",
        "    ## State variables\n",
        "    self.net_educativeness_exposure = net_educativeness_exposure\n",
        "    self.satisfaction = 1 / (1 + np.exp(-sensitivity * net_educativeness_exposure))\n",
        "    self.time_budget = time_budget\n",
        "\n",
        "    # Noise\n",
        "    self._observation_noise = observation_noise_stddev\n",
        "\n",
        "  # Al igual que con los documentos, se retorna la observación del estado del usuario, en este caso lo único público es su satisfacción\n",
        "  def create_observation(self):\n",
        "    \"\"\"User's state is not observable.\"\"\"\n",
        "    clip_low, clip_high = (-1.0 / (1.0 * self._observation_noise),\n",
        "                           1.0 / (1.0 * self._observation_noise))\n",
        "    noise = stats.truncnorm(\n",
        "        clip_low, clip_high, loc=0.0, scale=self._observation_noise).rvs()\n",
        "    noisy_sat = self.satisfaction + noise\n",
        "    return np.array([noisy_sat,])\n",
        "\n",
        "  # También hay que definir el espacio de las variables que se retornen de una observación\n",
        "  @staticmethod\n",
        "  def observation_space():\n",
        "    return spaces.Box(shape=(1,), dtype=np.float32, low=-2.0, high=2.0)\n",
        "  \n",
        "  # Función de score para usar en el modelo de selección del usuario: en este caso el usuario tenderá a elegir más contenido de memes\n",
        "  def score_document(self, doc_obs):\n",
        "    return 1 - doc_obs['educativeness']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V21OxBX0B3nH"
      },
      "source": [
        "### Sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA92tNtyQd_t"
      },
      "source": [
        "Clase que sampleará los usuarios para la simulación, en este caso hay muchos parámetros que quedarán hardcodeados, pero se puede hacer dinámico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4NYbjnEB3Fu"
      },
      "source": [
        "class LTSStaticUserSampler(user.AbstractUserSampler):\n",
        "  _state_parameters = None\n",
        "\n",
        "  def __init__(self,\n",
        "               user_ctor=LTSUserState,\n",
        "               memory_discount=0.9,\n",
        "               sensitivity=0.01,\n",
        "               innovation_stddev=0.05,\n",
        "               meme_mean=5.0,\n",
        "               meme_stddev=1.0,\n",
        "               educ_mean=4.0,\n",
        "               educ_stddev=1.0,\n",
        "               time_budget=60,\n",
        "               **kwargs):\n",
        "    self._state_parameters = {'memory_discount': memory_discount,\n",
        "                              'sensitivity': sensitivity,\n",
        "                              'innovation_stddev': innovation_stddev,\n",
        "                              'meme_mean': meme_mean,\n",
        "                              'meme_stddev': meme_stddev,\n",
        "                              'educ_mean': educ_mean,\n",
        "                              'educ_stddev': educ_stddev,\n",
        "                              'time_budget': time_budget\n",
        "                             }\n",
        "    super(LTSStaticUserSampler, self).__init__(user_ctor, **kwargs)\n",
        "\n",
        "  def sample_user(self):\n",
        "    starting_nee = ((self._rng.random_sample() - .5) *\n",
        "                    (1 / (1.0 - self._state_parameters['memory_discount'])))\n",
        "    self._state_parameters['net_educativeness_exposure'] = starting_nee\n",
        "    return self._user_ctor(**self._state_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x9OsvaqB9Pg"
      },
      "source": [
        "### Response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWq6dHamQob5"
      },
      "source": [
        "Clase que define como es la respuesta de un usuario al interactuar con un documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwvLS9wrB5Pu"
      },
      "source": [
        "class LTSResponse(user.AbstractResponse):\n",
        "  # The maximum degree of engagement.\n",
        "  MAX_ENGAGEMENT_MAGNITUDE = 100.0\n",
        "\n",
        "  def __init__(self, cluster_id, clicked=False, engagement=0.0):\n",
        "    self.clicked = clicked\n",
        "    self.engagement = engagement\n",
        "    self.cluster_id = cluster_id\n",
        "\n",
        "  # Se crea la observación: si dió o no click, cuanto tiempo vió el item y a que cluster pertenece.\n",
        "  def create_observation(self):\n",
        "    return {'click': int(self.clicked),\n",
        "            'engagement': np.array(self.engagement),\n",
        "            'cluster_id': self.cluster_id}\n",
        "\n",
        "  # Se define el espacio de estas variables\n",
        "  @classmethod\n",
        "  def response_space(cls):\n",
        "    # `engagement` feature range is [0, MAX_ENGAGEMENT_MAGNITUDE]\n",
        "    return spaces.Dict({\n",
        "        'click':\n",
        "            spaces.Discrete(2),\n",
        "        'engagement':\n",
        "            spaces.Box(\n",
        "                low=0.0,\n",
        "                high=cls.MAX_ENGAGEMENT_MAGNITUDE,\n",
        "                shape=tuple(),\n",
        "                dtype=np.float32),\n",
        "        'cluster_id':\n",
        "            spaces.Discrete(4)\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_niBbg0NuId"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32AW3hr9Q7VG"
      },
      "source": [
        "Finalmente se define el modelo del usuario, el cual se compone por las clases definidas anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfXKd4nZCDvZ"
      },
      "source": [
        "class LTSUserModel(user.AbstractUserModel):\n",
        "    def __init__(self, slate_size, seed=0):\n",
        "        super(LTSUserModel, self).__init__(LTSResponse, LTSStaticUserSampler(LTSUserState, seed=seed), slate_size)\n",
        "        self.choice_model = MultinomialLogitChoiceModel({})\n",
        "    \n",
        "    def is_terminal(self):\n",
        "        # Retorna un boolean si la sesión se terminó, ya que el user tiene una variable de tiempo disponible (time_budget)\n",
        "        return self._user_state.time_budget <= 0\n",
        "\n",
        "    def simulate_response(self, slate_documents):\n",
        "        # Lista con respuestas vacías a partir del slate\n",
        "        responses = [self._response_model_ctor(d.cluster_id) for d in slate_documents]\n",
        "        # Se usa el choice_model del user para saber a qué documento le hace click\n",
        "        self.choice_model.score_documents(self._user_state,\n",
        "                                          [doc.create_observation() for doc in slate_documents])\n",
        "        scores = self.choice_model.scores\n",
        "        selected_index = self.choice_model.choose_item()\n",
        "        # Se genera la respuesta para el item que se clickeó\n",
        "        self.generate_response(slate_documents[selected_index],\n",
        "                               responses[selected_index])\n",
        "        return responses\n",
        "\n",
        "    def generate_response(self, doc, response):\n",
        "        response.clicked = True\n",
        "        # Se interpola linealmente entre meme y educativo\n",
        "        engagement_loc = (doc.educativeness * self._user_state.meme_mean + (1 - doc.educativeness) * self._user_state.educ_mean)\n",
        "        engagement_loc *= self._user_state.satisfaction\n",
        "        engagement_scale = (doc.educativeness * self._user_state.meme_stddev + ((1 - doc.educativeness) * self._user_state.educ_stddev))\n",
        "        log_engagement = np.random.normal(loc=engagement_loc,\n",
        "                                          scale=engagement_scale)\n",
        "        response.engagement = np.exp(log_engagement)\n",
        "\n",
        "    # Función que hace update del estado del usuario\n",
        "    def update_state(self, slate_documents, responses):\n",
        "        for doc, response in zip(slate_documents, responses):\n",
        "            if response.clicked:\n",
        "                innovation = np.random.normal(scale=self._user_state.innovation_stddev)\n",
        "                net_educativeness_exposure = (self._user_state.memory_discount * self._user_state.net_educativeness_exposure - 2.0 * (doc.educativeness - 0.5) + innovation)\n",
        "                self._user_state.net_educativeness_exposure = net_educativeness_exposure\n",
        "                satisfaction = 1 / (1.0 + np.exp(-self._user_state.sensitivity * net_educativeness_exposure))\n",
        "                self._user_state.satisfaction = satisfaction\n",
        "                self._user_state.time_budget -= 1\n",
        "                return\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG0h-b0eqt3M"
      },
      "source": [
        "## Crear environment: parámetros\n",
        "* *slate_size*: Tamaño del set de items a presentar al usuario.\n",
        "* *num_candidates*: número de documentos presentes en la base de datos en cualquier momento de la simulación.\n",
        "* *resample_documents*: especifica si se vuelven a samplear los documentos desde la base de datos entre episodios de la simulación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUw9z6KB_QL"
      },
      "source": [
        "slate_size = 3\n",
        "num_candidates = 10\n",
        "ltsenv = environment.Environment(\n",
        "    LTSUserModel(slate_size),\n",
        "    LTSDocumentSampler(),\n",
        "    num_candidates,\n",
        "    slate_size,\n",
        "    resample_documents=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTzyMHe9rYj2"
      },
      "source": [
        "### Parámetro a optimizar: Engagement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODqAlrjrO2__"
      },
      "source": [
        "def clicked_engagement_reward(responses):\n",
        "    reward = 0.0\n",
        "    for response in responses:\n",
        "        if response.clicked:\n",
        "            reward += response.engagement\n",
        "    return reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swYsZBh7PAdm"
      },
      "source": [
        "# Instanciar environment\n",
        "lts_gym_env = recsim_gym.RecSimGymEnv(ltsenv, clicked_engagement_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLrJ4MWSPMyO"
      },
      "source": [
        "observation_0 = lts_gym_env.reset()\n",
        "print('Observation 0')\n",
        "print('Available documents')\n",
        "doc_strings = ['doc_id ' + key + \" educativeness \" + str(value) for key, value\n",
        "               in observation_0['doc'].items()]\n",
        "print('\\n'.join(doc_strings))\n",
        "print('Noisy user state observation')\n",
        "print(observation_0['user'])\n",
        "# \"Agente\" recomienda los primeros 3 documentos\n",
        "recommendation_slate_0 = [0, 1, 2]\n",
        "observation_1, reward, done, _ = lts_gym_env.step(recommendation_slate_0)\n",
        "print('Observation 1')\n",
        "print('Available documents')\n",
        "doc_strings = ['doc_id ' + key + \" educativeness \" + str(value) for key, value\n",
        "               in observation_1['doc'].items()]\n",
        "print('\\n'.join(doc_strings))\n",
        "rsp_strings = [str(response) for response in observation_1['response']]\n",
        "print('User responses to documents in the slate')\n",
        "print('\\n'.join(rsp_strings))\n",
        "print('Noisy user state observation')\n",
        "print(observation_1['user'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EvnGBRFqgLB"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BII0kzpcPOJY"
      },
      "source": [
        "from recsim import agent\n",
        "from recsim.agents.layers.abstract_click_bandit import AbstractClickBanditLayer\n",
        "from recsim.agents.layers.cluster_click_statistics import ClusterClickStatsLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmbbm7otSg42"
      },
      "source": [
        "Crearemos un agente simple que ordene los documentos de un tópico de acuerdo a su educativeness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k47hd5pdqkjk"
      },
      "source": [
        "class GreedyClusterAgent(agent.AbstractEpisodicRecommenderAgent):\n",
        "  def __init__(self, observation_space, action_space, cluster_id, pro_educ, **kwargs):\n",
        "    del observation_space\n",
        "    super(GreedyClusterAgent, self).__init__(action_space)\n",
        "    self._cluster_id = cluster_id\n",
        "    self.pro_educ = pro_educ\n",
        "\n",
        "  def step(self, reward, observation):\n",
        "    del reward\n",
        "    my_docs = []\n",
        "    my_doc_educativeness = []\n",
        "    for i, doc in enumerate(observation['doc'].values()):\n",
        "      if doc['cluster_id'] == self._cluster_id:\n",
        "        my_docs.append(i)\n",
        "        my_doc_educativeness.append(doc['educativeness'])\n",
        "    if not bool(my_docs):\n",
        "      return []\n",
        "    # Agregamos esta variable booleana para determinar si ordena los documentos de mayor a menor o al revés (algunos agentes preferirán recomendar los memes primero)\n",
        "    if self.pro_educ:\n",
        "        sorted_indices = np.argsort(my_doc_educativeness)[::-1]\n",
        "    else:\n",
        "        sorted_indices = np.argsort(my_doc_educativeness)\n",
        "    return list(np.array(my_docs)[sorted_indices])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IprHtI5TwEGm"
      },
      "source": [
        "# Obtenemos el número de tópicos disponibles\n",
        "num_topics = LTSDocument.observation_space()['cluster_id'].n\n",
        "# Creamos un agente para cada tópico\n",
        "base_agent_ctors = [functools.partial(GreedyClusterAgent, cluster_id=i, pro_educ=np.random.choice([True, False], 1)[0]) for i in range(num_topics)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUW3In1zwYaa"
      },
      "source": [
        "# Recsim posee clases que se pueden usar como \"capas\" en keras o pytorch, aquí usamos AbstractBanditLayer que recibe un conjunto de agents que trata como arms\n",
        "bandit_ctor = functools.partial(AbstractClickBanditLayer, arm_base_agent_ctors=base_agent_ctors)\n",
        "# Otra capa que se puede usar es ClusterClickStatsLayer la cual le pasa información del número de clicks que ha hecho el usuario a cada cluster\n",
        "cluster_bandit = ClusterClickStatsLayer(bandit_ctor,\n",
        "                                        lts_gym_env.observation_space,\n",
        "                                        lts_gym_env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaV0-YuFUEH1"
      },
      "source": [
        "Ejemplo de recomendación hecho por este cluster de bandits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWqSBCxjw2IP"
      },
      "source": [
        "observation0 = lts_gym_env.reset()\n",
        "slate = cluster_bandit.begin_episode(observation0)\n",
        "print(\"Cluster bandit slate 0:\")\n",
        "doc_list = list(observation0['doc'].values())\n",
        "for doc_position in slate:\n",
        "    print(doc_list[doc_position])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUA_EiXFUNPg"
      },
      "source": [
        "Agregaremos una función que toma los parámetros de la simulación y crea nuestro agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWO22Ldm0qYp"
      },
      "source": [
        "def create_agent(sess, environment, eval_mode, summary_writer=None):\n",
        "    kwargs = {\n",
        "        'observation_space': environment.observation_space,\n",
        "        'action_space': environment.action_space,\n",
        "        'summary_writer': summary_writer,\n",
        "        'eval_mode': eval_mode,\n",
        "    }\n",
        "    return ClusterClickStatsLayer(bandit_ctor, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08PkwPjI5cf8"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPKOmb_-w4Cu"
      },
      "source": [
        "tmp_base_dir = '/tmp/recsim/'\n",
        "lts_gym_env.reset()\n",
        "runner = runner_lib.TrainRunner(\n",
        "    base_dir=tmp_base_dir,\n",
        "    create_agent_fn=create_agent,\n",
        "    env=lts_gym_env,\n",
        "    episode_log_file=\"\",\n",
        "    max_training_steps=100,\n",
        "    num_iterations=20)\n",
        "runner.run_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRhd4y1t1OqK"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WshLZLs1OCI"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPVb_LSP002c"
      },
      "source": [
        "%tensorboard --logdir=/tmp/recsim/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLnC2g6E5ISA"
      },
      "source": [
        "# Actividades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hYgL5MN5MSq"
      },
      "source": [
        "### Actividad 1:\n",
        "\n",
        "Entrene por más episodios y describa lo que está ocurriendo con el agente y el usuario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enu8Kf565Lfm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s0gJp7s5h9a"
      },
      "source": [
        "### Actividad 2\n",
        "\n",
        "Explique con sus palabras cuál es la principal ventaja de utilizar una librería como recsim o recogym para Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFpSI7U-5jl6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlWGRYYiUsFR"
      },
      "source": [
        "### Actividad 3\n",
        "\n",
        "¿Cómo se podría mejorar la forma de modelar al usuario?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwO4JNR8Uu_A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}